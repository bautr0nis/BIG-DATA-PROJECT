{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4eb6b8-c8dd-4963-8592-cf5e2d25446d",
   "metadata": {},
   "source": [
    "## 1. Prepare Python environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7f743be-6211-4854-8bb0-9f909c01018a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:02:35.521972Z",
     "start_time": "2024-12-11T17:02:22.619632Z"
    }
   },
   "source": [
    "# Install custom Python packages\n",
    "!pip3 install pyeucountrycodes --quiet\n",
    "!pip3 install calplot --quiet\n",
    "!pip3 install gradio --quiet"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3 install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3 install --upgrade pip\u001B[0m\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip3 install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f7ec82b7-6a9f-46f2-bb84-c12b53837e55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:03:32.316052Z",
     "start_time": "2024-12-11T17:03:28.902062Z"
    }
   },
   "source": [
    "# Load required Python packages\n",
    "import os, re, requests, logging, calplot\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "from dask.multiprocessing import get\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from eu_country_codes import COUNTRY_CODES\n",
    "import gradio as gr\n",
    "import plotly.express as px"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/dask/dataframe/__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "16462eaf-438b-4210-9821-db54c393a633",
   "metadata": {},
   "source": [
    "## 2. Download data files"
   ]
  },
  {
   "cell_type": "code",
   "id": "890eabf2-2103-46c1-82a1-3c5795e0f31a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:03:35.729462Z",
     "start_time": "2024-12-11T17:03:35.724357Z"
    }
   },
   "source": [
    "# Function to download remote file to the disk\n",
    "def urlDownload(urlLink, showProgress = False):\n",
    "  with requests.get(urlLink, stream=True) as r:\n",
    "    fileSize = int(r.headers.get('Content-Length'))\n",
    "    fileName = r.headers.get('Content-Disposition').split(\"filename=\")[1]\n",
    "    if not os.path.exists(fileName) or os.path.getsize(fileName) != fileSize:\n",
    "      block_size = 1024\n",
    "      if showProgress:\n",
    "        print(f\"Downloading {fileName}\")\n",
    "        progress_bar = tqdm(total=fileSize, unit='iB', unit_scale=True)\n",
    "      with open(fileName, 'wb') as file:\n",
    "        for data in r.iter_content(block_size):\n",
    "          if showProgress:\n",
    "            progress_bar.update(len(data))\n",
    "          file.write(data)\n",
    "      if showProgress:\n",
    "        progress_bar.close()\n",
    "    return fileName"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "fdf5bf16-bae0-4627-a9fb-62074a84f0ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:03:59.590816Z",
     "start_time": "2024-12-11T17:03:38.563487Z"
    }
   },
   "source": [
    "# Download the newest data\n",
    "urlLocation = 'https://aqicn.org/data-platform/covid19/report/39374-7694ec07/'\n",
    "csvFile = urlDownload(urlLocation, showProgress=True)\n",
    "csvFile"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading waqi-covid19-airqualitydata-2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105M/105M [00:19<00:00, 5.30MiB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'waqi-covid19-airqualitydata-2024.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a4edb190-1575-4154-a4ec-3513bcc4d87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:04:02.867186Z",
     "start_time": "2024-12-11T17:04:02.857426Z"
    }
   },
   "source": [
    "# Create lists of year and quarter names\n",
    "yNames = [str(i) for i in range(2019, 2024)]\n",
    "qNames = [\"Q\" + str(i) for i in range(1, 5)]\n",
    "\n",
    "# Create a data frame with the url locations and year/quarter combinations\n",
    "DF = pd.DataFrame(list(product(yNames, qNames)),columns=['yNames', 'qNames'])\n",
    "DF.insert(loc=0, column='urlLocation', value=urlLocation)\n",
    "\n",
    "# Combine url location and year/quarter combinations into a single column\n",
    "DF = pd.DataFrame({'urlLocations': DF.agg(''.join, axis=1)})\n",
    "DF"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                         urlLocations\n",
       "0   https://aqicn.org/data-platform/covid19/report...\n",
       "1   https://aqicn.org/data-platform/covid19/report...\n",
       "2   https://aqicn.org/data-platform/covid19/report...\n",
       "3   https://aqicn.org/data-platform/covid19/report...\n",
       "4   https://aqicn.org/data-platform/covid19/report...\n",
       "5   https://aqicn.org/data-platform/covid19/report...\n",
       "6   https://aqicn.org/data-platform/covid19/report...\n",
       "7   https://aqicn.org/data-platform/covid19/report...\n",
       "8   https://aqicn.org/data-platform/covid19/report...\n",
       "9   https://aqicn.org/data-platform/covid19/report...\n",
       "10  https://aqicn.org/data-platform/covid19/report...\n",
       "11  https://aqicn.org/data-platform/covid19/report...\n",
       "12  https://aqicn.org/data-platform/covid19/report...\n",
       "13  https://aqicn.org/data-platform/covid19/report...\n",
       "14  https://aqicn.org/data-platform/covid19/report...\n",
       "15  https://aqicn.org/data-platform/covid19/report...\n",
       "16  https://aqicn.org/data-platform/covid19/report...\n",
       "17  https://aqicn.org/data-platform/covid19/report...\n",
       "18  https://aqicn.org/data-platform/covid19/report...\n",
       "19  https://aqicn.org/data-platform/covid19/report..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlLocations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://aqicn.org/data-platform/covid19/report...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6076123e-58b7-4f4c-9d89-49853f93dd3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:04:48.018874Z",
     "start_time": "2024-12-11T17:04:06.484790Z"
    }
   },
   "source": [
    "# Download legacy data (in parallel)\n",
    "DDF = dd.from_pandas(DF, npartitions=mp.cpu_count())\n",
    "csvFiles = DDF.apply(lambda x : urlDownload(x[0]), axis=1, meta=pd.Series(dtype=\"str\")).compute(scheduler='threads')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 106.14 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/y7_11shj52j5ym8tns5r5r0h0000gn/T/ipykernel_96750/2709540422.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  csvFiles = DDF.apply(lambda x : urlDownload(x[0]), axis=1, meta=pd.Series(dtype=\"str\")).compute(scheduler='threads')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 41.52 s\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "e594e670-69cf-4940-a08c-06a44cbea4a8",
   "metadata": {},
   "source": [
    "## 3. Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "id": "be0414f8-11ee-48ba-9565-41a71e9d2c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:04:58.095249Z",
     "start_time": "2024-12-11T17:04:57.498272Z"
    }
   },
   "source": [
    "# Define the columns to load\n",
    "meta_cols = ['Date', 'Country', 'City', 'Specie']\n",
    "main_column = 'median' # 'count', 'min', 'max', 'median', 'variance'\n",
    "selected_cols = meta_cols + [main_column]\n",
    "\n",
    "# Read the newest data file and skip the first 4 lines\n",
    "DF = pd.read_csv(csvFile, skiprows=4, usecols=selected_cols)\n",
    "\n",
    "# Leave EU data, rename main column to Value\n",
    "selectNL = DF['Country'] == 'NL'\n",
    "newTable = DF[selectNL].rename(columns={main_column: 'Value'})\n",
    "print(newTable)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date Country       City     Specie  Value\n",
      "1345408  2024-02-17      NL    Utrecht  wind-gust    0.9\n",
      "1345409  2024-03-08      NL    Utrecht  wind-gust    3.3\n",
      "1345410  2024-05-28      NL    Utrecht  wind-gust    2.6\n",
      "1345411  2024-06-30      NL    Utrecht  wind-gust    2.0\n",
      "1345412  2024-01-27      NL    Utrecht  wind-gust    0.5\n",
      "...             ...     ...        ...        ...    ...\n",
      "1383189  2024-09-28      NL  The Hague       pm10   13.0\n",
      "1383190  2024-10-20      NL  The Hague       pm10   10.0\n",
      "1383191  2024-11-05      NL  The Hague       pm10   27.0\n",
      "1383192  2024-11-16      NL  The Hague       pm10   15.0\n",
      "1383193  2024-11-29      NL  The Hague       pm10   17.0\n",
      "\n",
      "[37786 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "701d7344-04b0-4cec-a33a-4c011c0a3ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:05:02.500283Z",
     "start_time": "2024-12-11T17:05:00.140439Z"
    }
   },
   "source": [
    "# Read legacy data files (in parallel)\n",
    "fileNamesQ = [f for f in os.listdir('.') if re.match(r'^.*Q\\d.csv$', f)]\n",
    "DF = dd.compute(dd.read_csv(fileNamesQ, skiprows=4, usecols=selected_cols))[0]\n",
    "selectNL = DF['Country'] == 'NL'\n",
    "oldTable = DF[selectNL].rename(columns={main_column: 'Value'})\n",
    "print(oldTable)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 1.96 sms\n",
      "              Date Country       City     Specie  Value\n",
      "99767   2021-04-23      NL    Utrecht   humidity   64.6\n",
      "99768   2021-04-25      NL    Utrecht   humidity   60.1\n",
      "99769   2021-04-26      NL    Utrecht   humidity   60.5\n",
      "99770   2021-04-30      NL    Utrecht   humidity   86.1\n",
      "99771   2021-05-12      NL    Utrecht   humidity   76.5\n",
      "...            ...     ...        ...        ...    ...\n",
      "488233  2019-01-17      NL  The Hague  wind-gust   14.5\n",
      "488234  2019-01-26      NL  The Hague  wind-gust   18.0\n",
      "488235  2019-02-07      NL  The Hague  wind-gust   22.4\n",
      "488236  2019-02-19      NL  The Hague  wind-gust   14.2\n",
      "488237  2019-03-23      NL  The Hague  wind-gust    7.7\n",
      "\n",
      "[188031 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "75d141b6-df47-4e67-875a-ba3091d3c61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T17:05:04.476579Z",
     "start_time": "2024-12-11T17:05:04.320994Z"
    }
   },
   "source": [
    "# Append old (2018-2023) and new (2024) data tables, sort, remove duplicates\n",
    "DF = pd.concat([oldTable, newTable])\n",
    "dataTableEU = DF.sort_values(by=['Country', 'City', 'Date']).drop_duplicates()\n",
    "print(dataTableEU)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date Country       City       Specie   Value\n",
      "482829   2018-12-31      NL  Amsterdam          no2    10.1\n",
      "482923   2018-12-31      NL  Amsterdam   wind-speed     2.7\n",
      "482957   2018-12-31      NL  Amsterdam    wind-gust     6.5\n",
      "483094   2018-12-31      NL  Amsterdam  temperature     9.4\n",
      "483166   2018-12-31      NL  Amsterdam     humidity    91.6\n",
      "...             ...     ...        ...          ...     ...\n",
      "1347330  2024-12-11      NL    Utrecht     pressure  1025.4\n",
      "1347548  2024-12-11      NL    Utrecht   wind-speed     2.0\n",
      "1347879  2024-12-11      NL    Utrecht     humidity    87.6\n",
      "1348151  2024-12-11      NL    Utrecht          so2     0.1\n",
      "1348554  2024-12-11      NL    Utrecht         pm25    45.0\n",
      "\n",
      "[216106 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T16:29:31.188398Z",
     "start_time": "2024-12-15T16:29:30.907604Z"
    }
   },
   "cell_type": "code",
   "source": "dataTableEU.to_csv('output.csv', index=False)",
   "id": "bf13ca968c052400",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:12.356427Z",
     "start_time": "2024-12-16T19:37:12.291987Z"
    }
   },
   "cell_type": "code",
   "source": "dataTableEU = pd.read_csv('output.csv')",
   "id": "b2d8e0842a17a94c",
   "outputs": [],
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "id": "618a4fb7-237e-47ff-b703-cfe040a091ac",
   "metadata": {},
   "source": [
    "## 4. Check and select vars"
   ]
  },
  {
   "cell_type": "code",
   "id": "814b1156-6bcd-4fd0-9462-40564ed300ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:14.815996Z",
     "start_time": "2024-12-16T19:37:14.801191Z"
    }
   },
   "source": [
    "# Weather conditions and pollutants (PM10, PM2.5, NO2, Ozone, SO2, CO)\n",
    "\n",
    "# Calculate the proportion of each Species in the data table\n",
    "all_vars = 100 * pd.value_counts(dataTableEU.Specie) / len(dataTableEU)\n",
    "\n",
    "# Drop the variables that are not needed\n",
    "drop_weat = [] #['pressure', 'wind-speed', 'wind-gust', 'wind speed', 'wind gust', 'dew', 'precipitation'] # AR TIKRAI REIKIA SITUS PASALINT???!!!!\n",
    "drop_poll = [] #['wd', 'aqi', 'uvi', 'pm1', 'neph', 'mepaqi']\n",
    "keep_vars = set(all_vars.index) - set(drop_weat + drop_poll)\n",
    "\n",
    "# Create a new data table with the info on kept variables\n",
    "new_data_table = pd.DataFrame([all_vars[list(keep_vars)].sort_values(ascending=False)])\n",
    "new_data_table.style.hide(axis=\"index\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/y7_11shj52j5ym8tns5r5r0h0000gn/T/ipykernel_96750/2887189367.py:4: FutureWarning:\n",
      "\n",
      "pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x35762cbc0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_617b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_617b0_level0_col0\" class=\"col_heading level0 col0\" >no2</th>\n",
       "      <th id=\"T_617b0_level0_col1\" class=\"col_heading level0 col1\" >pm10</th>\n",
       "      <th id=\"T_617b0_level0_col2\" class=\"col_heading level0 col2\" >pressure</th>\n",
       "      <th id=\"T_617b0_level0_col3\" class=\"col_heading level0 col3\" >humidity</th>\n",
       "      <th id=\"T_617b0_level0_col4\" class=\"col_heading level0 col4\" >temperature</th>\n",
       "      <th id=\"T_617b0_level0_col5\" class=\"col_heading level0 col5\" >pm25</th>\n",
       "      <th id=\"T_617b0_level0_col6\" class=\"col_heading level0 col6\" >o3</th>\n",
       "      <th id=\"T_617b0_level0_col7\" class=\"col_heading level0 col7\" >wind-speed</th>\n",
       "      <th id=\"T_617b0_level0_col8\" class=\"col_heading level0 col8\" >wind-gust</th>\n",
       "      <th id=\"T_617b0_level0_col9\" class=\"col_heading level0 col9\" >so2</th>\n",
       "      <th id=\"T_617b0_level0_col10\" class=\"col_heading level0 col10\" >dew</th>\n",
       "      <th id=\"T_617b0_level0_col11\" class=\"col_heading level0 col11\" >co</th>\n",
       "      <th id=\"T_617b0_level0_col12\" class=\"col_heading level0 col12\" >wind gust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_617b0_row0_col0\" class=\"data row0 col0\" >10.246130</td>\n",
       "      <td id=\"T_617b0_row0_col1\" class=\"data row0 col1\" >10.242414</td>\n",
       "      <td id=\"T_617b0_row0_col2\" class=\"data row0 col2\" >10.143924</td>\n",
       "      <td id=\"T_617b0_row0_col3\" class=\"data row0 col3\" >10.143460</td>\n",
       "      <td id=\"T_617b0_row0_col4\" class=\"data row0 col4\" >10.142995</td>\n",
       "      <td id=\"T_617b0_row0_col5\" class=\"data row0 col5\" >9.786669</td>\n",
       "      <td id=\"T_617b0_row0_col6\" class=\"data row0 col6\" >9.636612</td>\n",
       "      <td id=\"T_617b0_row0_col7\" class=\"data row0 col7\" >9.395035</td>\n",
       "      <td id=\"T_617b0_row0_col8\" class=\"data row0 col8\" >8.758571</td>\n",
       "      <td id=\"T_617b0_row0_col9\" class=\"data row0 col9\" >5.006225</td>\n",
       "      <td id=\"T_617b0_row0_col10\" class=\"data row0 col10\" >3.873135</td>\n",
       "      <td id=\"T_617b0_row0_col11\" class=\"data row0 col11\" >2.245275</td>\n",
       "      <td id=\"T_617b0_row0_col12\" class=\"data row0 col12\" >0.379555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 156
  },
  {
   "cell_type": "code",
   "id": "1794a47c-4979-4731-bbdc-e490c1b01d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:15.622049Z",
     "start_time": "2024-12-16T19:37:15.594069Z"
    }
   },
   "source": [
    "# Descriptive statistics for daily values of selected variables\n",
    "selectedVars = ['temperature', 'humidity', 'no2', 'pm10', 'pressure', 'pm25', 'o3', 'wind-speed', 'wind-gust', 'so2', 'dew', 'co', 'wind gust']\n",
    "#selectedVars = ['no2', 'pm10','pm25', 'o3','so2','co']\n",
    "selectedIdx = dataTableEU['Specie'].isin(selectedVars)\n",
    "dataTableEU = dataTableEU[selectedIdx]\n",
    "dataTableEU.groupby('Specie')['Value'].describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               count         mean        std    min     25%     50%      75%  \\\n",
       "Specie                                                                         \n",
       "co            4833.0     2.539292   0.768974    0.7     2.0     2.5     3.00   \n",
       "dew           8337.0     7.701259   5.118997  -12.0     4.0     8.0    11.80   \n",
       "humidity     21834.0    79.341930  12.313332    1.0    72.2    82.0    88.70   \n",
       "no2          22055.0     8.060521   4.348462    0.9     5.0     7.1    10.10   \n",
       "o3           20743.0    20.292429   8.462526    0.1    15.2    21.0    25.95   \n",
       "pm10         22047.0    16.262303   6.623625    3.0    12.0    15.0    19.00   \n",
       "pm25         21066.0    35.896990  18.617160    1.0    22.0    31.0    47.00   \n",
       "pressure     21835.0  1013.546586  11.636400  858.1  1007.0  1014.5  1021.00   \n",
       "so2          10776.0     0.463827   1.107822    0.1     0.2     0.3     0.50   \n",
       "temperature  21833.0    11.916420   6.006536   -7.0     7.5    11.6    16.50   \n",
       "wind gust      817.0    10.087638   5.292520    0.6     6.2     9.5    13.30   \n",
       "wind-gust    18853.0     7.753610   4.527280    0.1     4.3     7.0    10.30   \n",
       "wind-speed   20223.0     3.266780   2.212676    0.1     1.7     2.8     4.30   \n",
       "\n",
       "                max  \n",
       "Specie               \n",
       "co              7.9  \n",
       "dew            21.0  \n",
       "humidity      100.0  \n",
       "no2            36.9  \n",
       "o3             64.9  \n",
       "pm10           64.0  \n",
       "pm25          158.0  \n",
       "pressure     1053.9  \n",
       "so2           102.1  \n",
       "temperature    33.8  \n",
       "wind gust      30.0  \n",
       "wind-gust      34.4  \n",
       "wind-speed     49.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specie</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>4833.0</td>\n",
       "      <td>2.539292</td>\n",
       "      <td>0.768974</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dew</th>\n",
       "      <td>8337.0</td>\n",
       "      <td>7.701259</td>\n",
       "      <td>5.118997</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.80</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>21834.0</td>\n",
       "      <td>79.341930</td>\n",
       "      <td>12.313332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>82.0</td>\n",
       "      <td>88.70</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no2</th>\n",
       "      <td>22055.0</td>\n",
       "      <td>8.060521</td>\n",
       "      <td>4.348462</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>10.10</td>\n",
       "      <td>36.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o3</th>\n",
       "      <td>20743.0</td>\n",
       "      <td>20.292429</td>\n",
       "      <td>8.462526</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>25.95</td>\n",
       "      <td>64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pm10</th>\n",
       "      <td>22047.0</td>\n",
       "      <td>16.262303</td>\n",
       "      <td>6.623625</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pm25</th>\n",
       "      <td>21066.0</td>\n",
       "      <td>35.896990</td>\n",
       "      <td>18.617160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pressure</th>\n",
       "      <td>21835.0</td>\n",
       "      <td>1013.546586</td>\n",
       "      <td>11.636400</td>\n",
       "      <td>858.1</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>1014.5</td>\n",
       "      <td>1021.00</td>\n",
       "      <td>1053.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so2</th>\n",
       "      <td>10776.0</td>\n",
       "      <td>0.463827</td>\n",
       "      <td>1.107822</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>102.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>21833.0</td>\n",
       "      <td>11.916420</td>\n",
       "      <td>6.006536</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>16.50</td>\n",
       "      <td>33.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind gust</th>\n",
       "      <td>817.0</td>\n",
       "      <td>10.087638</td>\n",
       "      <td>5.292520</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>13.30</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind-gust</th>\n",
       "      <td>18853.0</td>\n",
       "      <td>7.753610</td>\n",
       "      <td>4.527280</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>34.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind-speed</th>\n",
       "      <td>20223.0</td>\n",
       "      <td>3.266780</td>\n",
       "      <td>2.212676</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.30</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "cell_type": "code",
   "id": "4da0f1ca-40cf-4262-b5d9-bf7f9e3526e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:17.724071Z",
     "start_time": "2024-12-16T19:37:17.678256Z"
    }
   },
   "source": [
    "# 2021-10-03 Barcelona fix\n",
    "print(dataTableEU)\n",
    "dataTableEU = dataTableEU.groupby(['Date', 'Country', 'City', 'Specie'])[['Value']].mean().reset_index()\n",
    "print(dataTableEU)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Date Country       City       Specie   Value\n",
      "0       2018-12-31      NL  Amsterdam           co     2.5\n",
      "1       2018-12-31      NL  Amsterdam          dew     8.0\n",
      "2       2018-12-31      NL  Amsterdam     humidity    91.6\n",
      "3       2018-12-31      NL  Amsterdam          no2    10.1\n",
      "4       2018-12-31      NL  Amsterdam           o3    12.1\n",
      "...            ...     ...        ...          ...     ...\n",
      "215247  2024-12-11      NL    Utrecht     pressure  1025.4\n",
      "215248  2024-12-11      NL    Utrecht          so2     0.1\n",
      "215249  2024-12-11      NL    Utrecht  temperature     3.8\n",
      "215250  2024-12-11      NL    Utrecht    wind-gust     2.6\n",
      "215251  2024-12-11      NL    Utrecht   wind-speed     2.0\n",
      "\n",
      "[215252 rows x 5 columns]\n",
      "              Date Country       City       Specie   Value\n",
      "0       2018-12-31      NL  Amsterdam           co     2.5\n",
      "1       2018-12-31      NL  Amsterdam          dew     8.0\n",
      "2       2018-12-31      NL  Amsterdam     humidity    91.6\n",
      "3       2018-12-31      NL  Amsterdam          no2    10.1\n",
      "4       2018-12-31      NL  Amsterdam           o3    12.1\n",
      "...            ...     ...        ...          ...     ...\n",
      "215247  2024-12-11      NL    Utrecht     pressure  1025.4\n",
      "215248  2024-12-11      NL    Utrecht          so2     0.1\n",
      "215249  2024-12-11      NL    Utrecht  temperature     3.8\n",
      "215250  2024-12-11      NL    Utrecht    wind-gust     2.6\n",
      "215251  2024-12-11      NL    Utrecht   wind-speed     2.0\n",
      "\n",
      "[215252 rows x 5 columns]\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "cell_type": "markdown",
   "id": "d12dab2a-b940-42a7-a416-30730fb4505e",
   "metadata": {},
   "source": [
    "## 5. Pivot and calculate THI"
   ]
  },
  {
   "cell_type": "code",
   "id": "32bc5663-420e-4845-bf86-9ac00390d397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:37:22.587662Z",
     "start_time": "2024-12-16T19:37:22.530927Z"
    }
   },
   "source": [
    "# Create pivot table, calculate THI for each row, drop rows with missing THI values\n",
    "dataTableTHI = dataTableEU.pivot_table(index=['Date', 'Country', 'City'], columns='Specie', values='Value').reset_index()\n",
    "dataTableTHI[\"THI\"] = 0.8 * dataTableTHI.temperature + (dataTableTHI.humidity/100)*(dataTableTHI.temperature-14.4) + 46.4\n",
    "dataTableTHI = dataTableTHI.dropna(subset=[\"THI\"])\n",
    "print(dataTableTHI)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specie        Date Country        City   co  dew  humidity   no2    o3  pm10  \\\n",
      "0       2018-12-31      NL   Amsterdam  2.5  8.0      91.6  10.1  12.1  19.0   \n",
      "1       2018-12-31      NL       Breda  NaN  7.8      93.1   9.2   9.7  16.0   \n",
      "2       2018-12-31      NL   Dordrecht  NaN  NaN      97.0   7.8   9.5  15.0   \n",
      "3       2018-12-31      NL   Eindhoven  NaN  7.7      98.0   8.2   8.3  24.0   \n",
      "4       2018-12-31      NL   Groningen  NaN  8.0      91.0   6.1  16.5  23.0   \n",
      "...            ...     ...         ...  ...  ...       ...   ...   ...   ...   \n",
      "22050   2024-12-11      NL  Maastricht  NaN  2.0      89.0   6.0  11.8  15.0   \n",
      "22051   2024-12-11      NL    Nijmegen  NaN  NaN      84.0   6.0  15.7  12.0   \n",
      "22052   2024-12-11      NL   Rotterdam  NaN  1.5      86.0   5.6  17.3  18.0   \n",
      "22053   2024-12-11      NL   The Hague  NaN  NaN      89.5   4.3  18.5  18.0   \n",
      "22054   2024-12-11      NL     Utrecht  NaN  NaN      87.6   3.7  19.7  13.0   \n",
      "\n",
      "Specie  pm25  pressure  so2  temperature  wind gust  wind-gust  wind-speed  \\\n",
      "0       44.0    1031.4  0.2          9.4        NaN        6.5         2.7   \n",
      "1       46.0    1033.4  NaN          8.3        NaN        3.3         2.0   \n",
      "2       34.0    1032.9  NaN          8.8        NaN        4.1         1.8   \n",
      "3       59.0    1033.0  NaN          8.8        NaN        3.0         2.0   \n",
      "4       38.0    1031.0  NaN          8.8        NaN        8.5         3.5   \n",
      "...      ...       ...  ...          ...        ...        ...         ...   \n",
      "22050   57.0    1030.8  0.2          3.3        NaN        7.0         3.0   \n",
      "22051   38.0    1040.5  NaN          3.8        NaN        7.5         3.1   \n",
      "22052   57.0    1032.0  NaN          4.0        NaN        6.1         3.6   \n",
      "22053   57.0    1031.9  0.2          4.0        NaN       11.7         4.6   \n",
      "22054   45.0    1025.4  0.1          3.8        NaN        2.6         2.0   \n",
      "\n",
      "Specie      THI  \n",
      "0       49.3400  \n",
      "1       47.3609  \n",
      "2       48.0080  \n",
      "3       47.9520  \n",
      "4       48.3440  \n",
      "...         ...  \n",
      "22050   39.1610  \n",
      "22051   40.5360  \n",
      "22052   40.6560  \n",
      "22053   40.2920  \n",
      "22054   40.1544  \n",
      "\n",
      "[21832 rows x 17 columns]\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:57:50.451499Z",
     "start_time": "2024-12-16T19:57:50.392087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataTableAPI = dataTableEU.pivot_table(index=['Date', 'Country', 'City'], \n",
    "                                       columns='Specie', \n",
    "                                       values='Value').reset_index()\n",
    "\n",
    "# Apskaičiuojame API kaip MAX tarp teršalų, ignoruodami NaN\n",
    "dataTableAPI['API'] = dataTableAPI[['pm10', 'pm25', 'no2', 'o3', 'so2', 'co']].max(axis=1, skipna=True)\n",
    "\n",
    "# Nurodome dominuojantį teršalą, kuris lėmė API\n",
    "dataTableAPI['Dominant_Specie'] = dataTableAPI[['pm10', 'pm25', 'no2', 'o3', 'so2', 'co']].idxmax(axis=1)\n",
    "\n",
    "# Pašaliname eilutes, kuriose API yra NaN (jeigu visos reikšmės buvo NaN)\n",
    "dataTableAPI = dataTableAPI.dropna(subset=['API'])\n",
    "\n",
    "# Rezultatas\n",
    "print(dataTableAPI[['Date', 'City', 'API', 'Dominant_Specie']].head())"
   ],
   "id": "3b92030e0e548e73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specie        Date       City   API Dominant_Specie\n",
      "0       2018-12-31  Amsterdam  44.0            pm25\n",
      "1       2018-12-31      Breda  46.0            pm25\n",
      "2       2018-12-31  Dordrecht  34.0            pm25\n",
      "3       2018-12-31  Eindhoven  59.0            pm25\n",
      "4       2018-12-31  Groningen  38.0            pm25\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T19:41:30.104173Z",
     "start_time": "2024-12-16T19:41:30.100484Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataTableAPI['API'].mean())",
   "id": "fe6ec347d30f54ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.81244389027431\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "# Ribinės reikšmės ir API skalė\n",
    "dataTableAPI = dataTableEU.pivot_table(index=['Date', 'Country', 'City'],\n",
    "                                       columns='Specie', \n",
    "                                       values='Value').reset_index()\n",
    "\n",
    "\n",
    "thresholds = {\n",
    "    'pm10': [(0, 50), (50, 100)],   # Ribos ir API reikšmės intervalams\n",
    "    'pm25': [(0, 15), (15, 35)],\n",
    "    'no2':  [(0, 40), (40, 100)],\n",
    "    'so2':  [(0, 20), (20, 75)],\n",
    "    'o3':   [(0, 100), (100, 200)],\n",
    "    'co':   [(0, 4), (4, 10)]\n",
    "}\n",
    "\n",
    "# Funkcija API skaičiavimui\n",
    "def calculate_api(value, limits):\n",
    "    for (low, high), (api_low, api_high) in zip(limits, [(0, 50), (50, 100)]):\n",
    "        if low <= value <= high:\n",
    "            return (value - low) / (high - low) * (api_high - api_low) + api_low\n",
    "    return None  # Grąžina None, jei reikšmė nepatenka į ribas\n",
    "\n",
    "# Taikome API skaičiavimui kiekvienam teršalui\n",
    "for Specie, limits in thresholds.items():\n",
    "    dataTableAPI[f'API_{Specie}'] = dataTableAPI[Specie].apply(lambda x: calculate_api(x, limits))\n",
    "\n",
    "# Bendras API – didžiausia reikšmė tarp visų teršalų\n",
    "dataTableAPI['API'] = dataTableAPI[[f'API_{p}' for p in thresholds.keys()]].max(axis=1)\n",
    "\n",
    "# Pridėti stulpelį, kuris nurodo teršalą (Specie), atsakingą už didžiausią API reikšmę\n",
    "dataTableAPI['Dominant_Specie'] = dataTableAPI[[f'API_{p}' for p in thresholds.keys()]].idxmax(axis=1).str.replace('API_', '')\n",
    "\n",
    "# Patikriname galutinę lentelę\n",
    "print(dataTableAPI[['Date', 'City', 'API', 'Dominant_Specie']].head())\n",
    "'''\n"
   ],
   "id": "13e1c6e5d3c46641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea3f4c2fb19b6039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:14:35.989242Z",
     "start_time": "2024-12-16T20:14:35.981806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pridėti stulpelį su sąlyga\n",
    "WHO_limit = 50\n",
    "dataTableAPI['Above_WHO'] = dataTableAPI['API'] > WHO_limit\n",
    "\n",
    "# Suskaičiuoti, kiek yra 'True' reikšmių (dienos viršijančios ribą)\n",
    "true_count = dataTableAPI['Above_WHO'].sum()\n",
    "\n",
    "# Apskaičiuoti vidutinę API reikšmę\n",
    "mean_api = dataTableAPI['API'].mean()\n",
    "\n",
    "# Rezultatų išvedimas\n",
    "print(f\"Viršijimų skaičius (Above_WHO = True): {true_count}\")\n",
    "print(f\"Vidutinė API reikšmė: {mean_api:.2f}\")"
   ],
   "id": "bc51bc9deae16415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viršijimų skaičius (Above_WHO = True): 4495\n",
      "Vidutinė API reikšmė: 36.81\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "cell_type": "markdown",
   "id": "1b4003ce-19da-45bc-9e87-55535be74920",
   "metadata": {},
   "source": "## 6. API statistics and plots"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:30:02.326893Z",
     "start_time": "2024-12-16T20:30:02.268901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Data Preparation ---\n",
    "dataTableAPI['Date'] = pd.to_datetime(dataTableAPI['Date'])\n",
    "dataTableAPI['Year'] = dataTableAPI['Date'].dt.year\n",
    "dataTableAPI['Month'] = dataTableAPI['Date'].dt.to_period(\"M\").astype(str)\n",
    "dataTableAPI['Week'] = dataTableAPI['Date'].dt.to_period(\"W\").astype(str)\n",
    "\n",
    "# Variables for selection\n",
    "selectedVars = ['temperature', 'humidity', 'no2', 'pm10', 'pressure', 'pm25',\n",
    "                'o3', 'wind-speed', 'wind-gust', 'so2', 'dew', 'co']\n",
    "\n",
    "# --- Descriptive Statistics Function ---\n",
    "def descriptive_stats_all(myYear):\n",
    "    \"\"\"Calculate descriptive statistics for API grouped by City.\"\"\"\n",
    "    filtered_data = dataTableAPI.copy()\n",
    "    if myYear:\n",
    "        filtered_data = filtered_data[filtered_data['Year'].isin(myYear)]\n",
    "    \n",
    "    stats = filtered_data.groupby('City')['API'].describe().reset_index()\n",
    "    stats = stats.rename(columns={\n",
    "        \"count\": \"Count\", \"mean\": \"Mean\", \"std\": \"Std Dev\",\n",
    "        \"min\": \"Min\", \"25%\": \"25%\", \"50%\": \"Median\",\n",
    "        \"75%\": \"75%\", \"max\": \"Max\"\n",
    "    })\n",
    "    return stats\n",
    "\n",
    "# --- Monthly API Trend Line Chart ---\n",
    "def monthly_api_trend_all(myYear):\n",
    "    \"\"\"Create a line chart showing monthly API trends for all cities.\"\"\"\n",
    "    filtered_data = dataTableAPI.copy()\n",
    "    if myYear:\n",
    "        filtered_data = filtered_data[filtered_data['Year'].isin(myYear)]\n",
    "    \n",
    "    monthly_trend = filtered_data.groupby(['City', 'Month'])['API'].mean().reset_index()\n",
    "    monthly_trend['Month'] = pd.to_datetime(monthly_trend['Month'])  # Convert to datetime\n",
    "    \n",
    "    # Plot line chart with quarterly labels\n",
    "    fig = px.line(monthly_trend, x='Month', y='API', color='City',\n",
    "                  title=\"Monthly API Trend for All Cities\",\n",
    "                  labels={\"API\": \"Average API\", \"Month\": \"Month\"})\n",
    "    fig.update_xaxes(\n",
    "        tickformat=\"%Y-Q%q\",  # Quarterly labels\n",
    "        dtick=\"M3\",           # Every 3 months\n",
    "        tickangle=45\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# --- Line Chart for Selected Variables with Aggregation ---\n",
    "def line_chart_selected_vars(myVars, myYear, aggregation):\n",
    "    \"\"\"Plot line charts for selected variables with aggregation (weekly/monthly).\"\"\"\n",
    "    if not myVars:\n",
    "        return px.line(title=\"No variables selected\")\n",
    "    \n",
    "    filtered_data = dataTableAPI.copy()\n",
    "    if myYear:\n",
    "        filtered_data = filtered_data[filtered_data['Year'].isin(myYear)]\n",
    "    \n",
    "    # Apply aggregation\n",
    "    if aggregation == \"Weekly\":\n",
    "        filtered_data['Time'] = filtered_data['Date'].dt.to_period(\"W\").apply(lambda x: x.start_time)\n",
    "    else:  # Monthly\n",
    "        filtered_data['Time'] = filtered_data['Date'].dt.to_period(\"M\").apply(lambda x: x.start_time)\n",
    "    \n",
    "    # Melt the data to long format for plotting\n",
    "    melted_data = filtered_data.melt(id_vars=['Time'], value_vars=myVars, \n",
    "                                     var_name='Variable', value_name='Value')\n",
    "    \n",
    "    # Ensure Value column is numeric\n",
    "    melted_data = melted_data[pd.to_numeric(melted_data['Value'], errors='coerce').notnull()]\n",
    "    melted_data['Value'] = pd.to_numeric(melted_data['Value'])\n",
    "    \n",
    "    # Aggregate by Time and Variable\n",
    "    aggregated_data = melted_data.groupby(['Time', 'Variable']).mean().reset_index()\n",
    "    \n",
    "    # Plot the line chart\n",
    "    fig = px.line(aggregated_data, x='Time', y='Value', color='Variable',\n",
    "                  title=f\"Line Chart for Selected Variables ({aggregation} Aggregation)\")\n",
    "    fig.update_xaxes(tickformat=\"%Y-%m\" if aggregation == \"Monthly\" else \"%Y-%U\", tickangle=45)\n",
    "    return fig\n",
    "\n",
    "def weekly_api_plot(cities, years, plot_type):\n",
    "    \"\"\"Generate API analysis for days of the week.\"\"\"\n",
    "    print(\"Before Filtering API Mean:\", dataTableAPI['API'].mean())\n",
    "\n",
    "    # Filtruojame duomenis pagal pasirinktus miestus\n",
    "    if isinstance(cities, list) and cities:  # Patikriname, ar miestai buvo pasirinkti\n",
    "        filtered_data = dataTableAPI[dataTableAPI['City'].isin(cities)]\n",
    "    else:\n",
    "        filtered_data = dataTableAPI.copy()  # Jei miestai nepasirinkti, naudojame visus duomenis\n",
    "    \n",
    "    print(f\"After Filtering by Cities {cities} API Mean:\", filtered_data['API'].mean())\n",
    "\n",
    "    # Filtruojame pagal metus, jei pasirinkti\n",
    "    if years:\n",
    "        filtered_data = filtered_data[filtered_data['Year'].isin(years)]\n",
    "        print(f\"After Filtering by Years {years} API Mean:\", filtered_data['API'].mean())\n",
    "\n",
    "    # Išfiltruojame trūkstamas API reikšmes\n",
    "    filtered_data = filtered_data.dropna(subset=['API'])\n",
    "    print(\"After Dropping NaNs API Mean:\", filtered_data['API'].mean())\n",
    "\n",
    "    # Pridedame savaitės dieną\n",
    "    filtered_data['DayOfWeek'] = filtered_data['Date'].dt.day_name()\n",
    "\n",
    "    if plot_type == \"calplot\":\n",
    "        # Kalendoriaus grafikas\n",
    "        pdTimeSeries = pd.Series(filtered_data['API'].values, index=pd.DatetimeIndex(filtered_data['Date']))\n",
    "        cp = calplot.calplot(pdTimeSeries, dropzero=True, cmap='coolwarm',\n",
    "                             yearlabel_kws={'color': 'black', 'fontsize': 9})\n",
    "        return cp[0]\n",
    "    else:\n",
    "        # Grupavimas vidurkiui\n",
    "        day_avg = filtered_data.groupby('DayOfWeek')['API'].mean().reset_index()\n",
    "        print(\"Grouped Weekly Averages:\\n\", day_avg)\n",
    "\n",
    "        # Užtikriname savaitės dienų tvarką\n",
    "        day_avg['DayOfWeek'] = pd.Categorical(day_avg['DayOfWeek'], \n",
    "                                              categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', \n",
    "                                                          'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "        day_avg = day_avg.sort_values('DayOfWeek')\n",
    "        \n",
    "        # Dinaminės spalvų skalės ribos\n",
    "        overall_mean = day_avg['API'].mean()\n",
    "        min_val = overall_mean - 10\n",
    "        max_val = overall_mean + 10\n",
    "        \n",
    "        # Heatmap grafikas\n",
    "        fig = px.imshow(\n",
    "            day_avg[['API']].T,  \n",
    "            labels=dict(x=\"Day of the Week\", y=\"Average API\", color=\"Average API\"),\n",
    "            x=day_avg['DayOfWeek'],\n",
    "            color_continuous_scale=\"viridis\",\n",
    "            zmin=min_val,\n",
    "            zmax=max_val\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Weekly API Heatmap\",\n",
    "            xaxis_title=\"Day of the Week\",\n",
    "            yaxis_title=\"\",\n",
    "            coloraxis_colorbar=dict(title=\"Average API\")\n",
    "        )\n",
    "        return fig"
   ],
   "id": "235ca699fdb1d44e",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:04:19.156853Z",
     "start_time": "2024-12-16T20:04:19.149346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Papildome WHO Threshold analizę, kad pridėtume miestus iš city_who_summary\n",
    "city_coords = {\n",
    "    \"Amsterdam\": [52.3676, 4.9041],\n",
    "    \"Breda\": [51.5719, 4.7683],\n",
    "    \"Dordrecht\": [51.8133, 4.6901],\n",
    "    \"Eindhoven\": [51.4416, 5.4697],\n",
    "    \"Groningen\": [53.2194, 6.5665],\n",
    "    \"Haarlem\": [52.3874, 4.6462],\n",
    "    \"Maastricht\": [50.8514, 5.6910],\n",
    "    \"Nijmegen\": [51.8126, 5.8372],\n",
    "    \"Rotterdam\": [51.9225, 4.4792],\n",
    "    \"The Hague\": [52.0705, 4.3007],\n",
    "    \"Utrecht\": [52.0907, 5.1214]\n",
    "}\n",
    "\n",
    "def who_threshold_map(years):\n",
    "    \"\"\"Generates a bubble map showing days above WHO threshold for multiple years.\"\"\"\n",
    "    # Ensure 'years' is a list\n",
    "    if not isinstance(years, list):\n",
    "        years = [years]\n",
    "    \n",
    "    # Filter data for the selected years\n",
    "    filtered_data = dataTableAPI[dataTableAPI['Year'].isin(years)]\n",
    "    \n",
    "    # Calculate Days Above WHO and Average API\n",
    "    days_above = filtered_data[filtered_data['Above_WHO']].groupby('City')['Above_WHO'].sum()\n",
    "    avg_api = filtered_data.groupby('City')['API'].mean()  # Use all rows for API average\n",
    "\n",
    "    # Combine into a summary table\n",
    "    city_summary = pd.concat([days_above, avg_api], axis=1).reset_index()\n",
    "    city_summary.columns = ['City', 'Days_Above_WHO', 'Average_API']\n",
    "    \n",
    "    # Fill missing values for cities without Above_WHO days\n",
    "    city_summary['Days_Above_WHO'] = city_summary['Days_Above_WHO'].fillna(0)\n",
    "    \n",
    "    # Add coordinates from city_coords\n",
    "    city_summary['Latitude'] = city_summary['City'].apply(lambda x: city_coords.get(x, [None, None])[0])\n",
    "    city_summary['Longitude'] = city_summary['City'].apply(lambda x: city_coords.get(x, [None, None])[1])\n",
    "    \n",
    "    # Filter out cities without coordinates\n",
    "    city_summary = city_summary.dropna(subset=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Min-Max Scaling for bubble sizes\n",
    "    min_days = city_summary['Days_Above_WHO'].min()\n",
    "    max_days = city_summary['Days_Above_WHO'].max()\n",
    "    city_summary['Scaled_Size'] = city_summary['Days_Above_WHO'].apply(\n",
    "        lambda x: 10 + 40 * (x - min_days) / (max_days - min_days) if max_days > min_days else 20\n",
    "    )\n",
    "\n",
    "    # Create a scatter mapbox with scaled circles\n",
    "    fig = px.scatter_mapbox(\n",
    "        city_summary,\n",
    "        lat='Latitude',\n",
    "        lon='Longitude',\n",
    "        size='Scaled_Size',  # Use scaled size\n",
    "        color='Average_API',\n",
    "        size_max=50,\n",
    "        zoom=6,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=f\"Cities with WHO Threshold Exceedance ({', '.join(map(str, years))})\",\n",
    "        hover_name='City',\n",
    "        hover_data={'Days_Above_WHO': True, 'Average_API': True, 'Latitude': False, 'Longitude': False}\n",
    "    )\n",
    "    \n",
    "    # Add text annotations for days above WHO threshold\n",
    "    for i, row in city_summary.iterrows():\n",
    "        fig.add_trace(\n",
    "            go.Scattermapbox(\n",
    "                lat=[row['Latitude']],\n",
    "                lon=[row['Longitude']],\n",
    "                mode='text',\n",
    "                text=[str(row['Days_Above_WHO'])],  # Days Above WHO value\n",
    "                textfont=dict(size=12, color='black'),\n",
    "                showlegend=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return fig"
   ],
   "id": "df0b4427d255607",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:04:19.466191Z",
     "start_time": "2024-12-16T20:04:19.460933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Function to Identify Dominant Pollutant ---\n",
    "def dominant_pollutant_summary(years):\n",
    "    \"\"\"Apskaičiuoja dažniausiai dominuojantį teršalą kiekvienais metais.\"\"\"\n",
    "    # Filtruojame duomenis pagal pasirinktus metus\n",
    "    filtered_data = dataTableAPI[dataTableAPI['Year'].isin(years)]\n",
    "    \n",
    "    # Identifikuojame teršalą, kuris sudaro API (MAX)\n",
    "    pollutants = ['no2', 'pm10', 'pm25', 'o3', 'so2', 'co']\n",
    "    filtered_data['Dominant_Pollutant'] = filtered_data[pollutants].idxmax(axis=1).str.upper()\n",
    "\n",
    "    # Skaičiuojame dažnius kiekvieno teršalo\n",
    "    dominant_counts = filtered_data['Dominant_Pollutant'].value_counts().reset_index()\n",
    "    dominant_counts.columns = ['Pollutant', 'Frequency']\n",
    "\n",
    "    # Pie chart\n",
    "    fig = px.pie(\n",
    "        dominant_counts,\n",
    "        names='Pollutant',\n",
    "        values='Frequency',\n",
    "        title=\"Frequency of Dominant Pollutants Contributing to API\",\n",
    "        color='Pollutant',\n",
    "        color_discrete_sequence=px.colors.sequential.RdBu\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def dominant_pollutant_trend(years):\n",
    "    \"\"\"Generuoja stacked bar chart, kaip keitėsi dominuojantys teršalai per metus.\"\"\"\n",
    "    # Filtruojame duomenis pagal pasirinktus metus\n",
    "    filtered_data = dataTableAPI[dataTableAPI['Year'].isin(years)]\n",
    "    \n",
    "    # Identifikuojame dominuojantį teršalą\n",
    "    pollutants = ['no2', 'pm10', 'pm25', 'o3', 'so2', 'co']\n",
    "    filtered_data['Dominant_Pollutant'] = filtered_data[pollutants].idxmax(axis=1).str.upper()\n",
    "\n",
    "    # Grupavimas pagal metus ir dominuojantį teršalą\n",
    "    trend_data = filtered_data.groupby(['Year', 'Dominant_Pollutant']).size().reset_index(name='Count')\n",
    "\n",
    "    # Stacked Bar Chart\n",
    "    fig = px.bar(\n",
    "        trend_data,\n",
    "        x='Year',\n",
    "        y='Count',\n",
    "        color='Dominant_Pollutant',\n",
    "        title=\"Trend of Dominant Pollutants Over Years\",\n",
    "        labels={'Count': 'Frequency', 'Year': 'Year'},\n",
    "        barmode='stack',\n",
    "        color_discrete_sequence=px.colors.sequential.RdBu\n",
    "    )\n",
    "    return fig"
   ],
   "id": "8b0a428fd7bbe92e",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:04:19.826747Z",
     "start_time": "2024-12-16T20:04:19.823975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pearson_correlation_analysis(selected_vars):\n",
    "    \"\"\"\n",
    "    Skaičiuoja Pearson koreliacijos koeficientus tarp API ir pasirinktų meteorologinių kintamųjų.\n",
    "    \"\"\"\n",
    "    # Filtruojame tik reikalingus kintamuosius\n",
    "    correlation_data = dataTableAPI[['API'] + selected_vars].dropna()\n",
    "\n",
    "    # Skaičiuojame koreliacijos matricą\n",
    "    corr_matrix = correlation_data.corr(method='pearson')\n",
    "\n",
    "    # Atliekame p-value reikšmingumo testą\n",
    "    p_values = correlation_data.corr(method=lambda x, y: stats.pearsonr(x, y)[1]) - np.eye(len(correlation_data.columns))\n",
    "\n",
    "    # Vizualizuojame koreliacijos matricą kaip heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    plt.title(\"Pearson Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Gražiname matplotlib objektą kaip išvestį\n",
    "    return plt, corr_matrix, p_values"
   ],
   "id": "8ee324861b922fd7",
   "outputs": [],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:04:20.213860Z",
     "start_time": "2024-12-16T20:04:20.198825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, normaltest\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- Data Preparation ---\n",
    "dataTableAPI['Date'] = pd.to_datetime(dataTableAPI['Date'])\n",
    "numeric_columns = dataTableAPI.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "weekly_data = (\n",
    "    dataTableAPI[['Date'] + numeric_columns]  # Paimame tik skaitinius stulpelius ir datą\n",
    "    .resample('W', on='Date')\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "weekly_data[numeric_columns] = weekly_data[numeric_columns].interpolate(method='linear')\n",
    "\n",
    "# 1. Stationarity Check (ADF Test)\n",
    "def check_stationarity():\n",
    "    result = adfuller(weekly_data['API'].dropna())\n",
    "    output = pd.DataFrame({\n",
    "        'Test Statistic': [result[0]],\n",
    "        'P-Value': [result[1]],\n",
    "        'Critical Values': [result[4]]\n",
    "    })\n",
    "    return output\n",
    "\n",
    "# 2. Distribution Check\n",
    "def check_distribution():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(weekly_data['API'].dropna(), bins=20, density=True, alpha=0.6, color='gray')\n",
    "    weekly_data['API'].dropna().plot(kind='kde', color='red', ax=ax)\n",
    "    ax.set_title(\"API Distribution Check\")\n",
    "    ax.set_xlabel(\"API\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# 3. Time Series Decomposition\n",
    "def decompose_time_series():\n",
    "    # Decompose with period=52\n",
    "    decomposition = seasonal_decompose(weekly_data['API'], model='additive', period=52)\n",
    "    \n",
    "    # Extract components and interpolate missing values\n",
    "    trend = decomposition.trend.interpolate(method='linear')  # Interpolate missing trend values\n",
    "    seasonal = decomposition.seasonal\n",
    "    residual = decomposition.resid.interpolate(method='linear')  # Interpolate missing residuals\n",
    "\n",
    "    # Create subplots for components\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1, \n",
    "        subplot_titles=(\"Trend\", \"Seasonal\", \"Residuals\"), \n",
    "        shared_xaxes=True\n",
    "    )\n",
    "    \n",
    "    # Trend Component\n",
    "    fig.add_trace(go.Scatter(x=weekly_data['Date'], y=trend, name=\"Trend\", line=dict(color='blue')), row=1, col=1)\n",
    "    \n",
    "    # Seasonal Component\n",
    "    fig.add_trace(go.Scatter(x=weekly_data['Date'], y=seasonal, name=\"Seasonal\", line=dict(color='red')), row=2, col=1)\n",
    "    \n",
    "    # Residuals Component\n",
    "    fig.add_trace(go.Scatter(x=weekly_data['Date'], y=residual, name=\"Residuals\", line=dict(color='green')), row=3, col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Time Series Decomposition\",\n",
    "        height=800,\n",
    "        xaxis_title=\"Date\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# 4. ACF and PACF Plots\n",
    "def plot_acf_pacf():\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    plot_acf(weekly_data['API'].dropna(), ax=axes[0], lags=20, title=\"Autocorrelation Function (ACF)\")\n",
    "    plot_pacf(weekly_data['API'].dropna(), ax=axes[1], lags=20, title=\"Partial Autocorrelation Function (PACF)\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n"
   ],
   "id": "bbdb091fd47f71f4",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T20:40:52.825954Z",
     "start_time": "2024-12-17T20:40:52.814931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# --- Modeliai Prognozavimui ---\n",
    "def forecast_models(horizon):\n",
    "    global forecast_data\n",
    "    forecast_data = {}\n",
    "    train = weekly_data.copy()\n",
    "    \n",
    "    # Generate future dates\n",
    "    last_date = train['Date'].iloc[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(weeks=1), periods=horizon, freq='W')\n",
    "    \n",
    "    # Naive Forecast\n",
    "    naive_forecast = np.full(horizon, train['API'].iloc[-1])\n",
    "    forecast_data['Naive'] = (future_dates, naive_forecast)\n",
    "    \n",
    "    # Holt-Winters\n",
    "    model_hw = ExponentialSmoothing(train['API'], seasonal='add', seasonal_periods=52).fit()\n",
    "    forecast_hw = model_hw.forecast(horizon)\n",
    "    forecast_data['Holt-Winter'] = (future_dates, forecast_hw)\n",
    "    \n",
    "    # ARIMA\n",
    "    model_arima = ARIMA(train['API'], order=(2, 1, 2)).fit()\n",
    "    forecast_arima = model_arima.forecast(horizon)\n",
    "    forecast_data['ARIMA'] = (future_dates, forecast_arima)\n",
    "    \n",
    "    # Prophet\n",
    "    prophet_df = train.rename(columns={\"Date\": \"ds\", \"API\": \"y\"})\n",
    "    model_prophet = Prophet()\n",
    "    model_prophet.fit(prophet_df)\n",
    "    future = model_prophet.make_future_dataframe(periods=horizon, freq='W')\n",
    "    forecast_prophet = model_prophet.predict(future)\n",
    "    future_forecast = forecast_prophet['yhat'][-horizon:]  # Only future predictions\n",
    "    forecast_data['Prophet'] = (future_dates, future_forecast)\n",
    "    \n",
    "    # Results Table\n",
    "    return pd.DataFrame({\n",
    "        'Naive': [sqrt(mean_squared_error(train['API'].iloc[-horizon:], naive_forecast))],\n",
    "        'Holt-Winter': [sqrt(mean_squared_error(train['API'].iloc[-horizon:], forecast_hw))],\n",
    "        'ARIMA': [sqrt(mean_squared_error(train['API'].iloc[-horizon:], forecast_arima))],\n",
    "        'Prophet': [sqrt(mean_squared_error(train['API'].iloc[-horizon:], future_forecast))],\n",
    "    }, index=[\"RMSE\"])\n",
    "\n",
    "# --- Grafikai Prognozėms ---\n",
    "def plot_forecast(model_name, horizon=12):\n",
    "    train = weekly_data.copy()\n",
    "    future_dates = pd.date_range(start=train['Date'].iloc[-1] + pd.Timedelta(weeks=1), periods=horizon, freq='W')\n",
    "    \n",
    "    # Initialize plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Training Data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train['Date'], y=train['API'], mode='lines', name='Training Data'\n",
    "    ))\n",
    "\n",
    "    # Forecast Data (Future Values)\n",
    "    if model_name in forecast_data:\n",
    "        dates, forecast_values = forecast_data[model_name]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dates, y=forecast_values, mode='lines', name=f'{model_name} Forecast'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Forecast Plot for {model_name} - {horizon} Weeks Ahead\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"API\",\n",
    "        height=500\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def backtesting_model():\n",
    "    horizons = [4, 12, 24]\n",
    "    results = []\n",
    "\n",
    "    for h in horizons:\n",
    "        result = forecast_models(h)\n",
    "        results.append(result.T)\n",
    "\n",
    "    combined = pd.concat(results, axis=1)\n",
    "    combined.columns = ['Horizon 4', 'Horizon 12', 'Horizon 24']\n",
    "    return combined"
   ],
   "id": "dd9429179dfca339",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T20:04:21.311661Z",
     "start_time": "2024-12-16T20:04:21.308608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_models(horizon):\n",
    "    \"\"\"Apskaičiuoja RMSE ir MAE kiekvienam modeliui.\"\"\"\n",
    "    train = weekly_data[:-horizon]\n",
    "    test = weekly_data[-horizon:]\n",
    "    actual = test['API'].reset_index(drop=True)\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    # Naive Forecast\n",
    "    naive_forecast = np.full(horizon, train['API'].iloc[-1])\n",
    "    models['Naive'] = {\n",
    "        'RMSE': mean_squared_error(actual, naive_forecast, squared=False),\n",
    "        'MAE': mean_absolute_error(actual, naive_forecast)\n",
    "    }\n",
    "\n",
    "    # Holt-Winters\n",
    "    model_hw = ExponentialSmoothing(train['API'], seasonal='add', seasonal_periods=52).fit()\n",
    "    forecast_hw = model_hw.forecast(horizon)\n",
    "    models['Holt-Winter'] = {\n",
    "        'RMSE': mean_squared_error(actual, forecast_hw, squared=False),\n",
    "        'MAE': mean_absolute_error(actual, forecast_hw)\n",
    "    }\n",
    "\n",
    "    # ARIMA\n",
    "    model_arima = ARIMA(train['API'], order=(2,1,2)).fit()\n",
    "    forecast_arima = model_arima.forecast(horizon)\n",
    "    models['ARIMA'] = {\n",
    "        'RMSE': mean_squared_error(actual, forecast_arima, squared=False),\n",
    "        'MAE': mean_absolute_error(actual, forecast_arima)\n",
    "    }\n",
    "\n",
    "    # Prophet\n",
    "    prophet_df = train.rename(columns={\"Date\": \"ds\", \"API\": \"y\"})\n",
    "    model_prophet = Prophet()\n",
    "    model_prophet.fit(prophet_df)\n",
    "    future = model_prophet.make_future_dataframe(periods=horizon, freq='W')\n",
    "    forecast_prophet = model_prophet.predict(future)['yhat'][-horizon:]\n",
    "    models['Prophet'] = {\n",
    "        'RMSE': mean_squared_error(actual, forecast_prophet, squared=False),\n",
    "        'MAE': mean_absolute_error(actual, forecast_prophet)\n",
    "    }\n",
    "\n",
    "    # Konvertuojame į tinkamą formatą\n",
    "    comparison_data = [{'Model': model, 'RMSE': values['RMSE'], 'MAE': values['MAE']}\n",
    "                       for model, values in models.items()]\n",
    "    return comparison_data"
   ],
   "id": "e08ecd12ced37a7d",
   "outputs": [],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T20:52:46.705994Z",
     "start_time": "2024-12-17T20:52:46.699200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "# --- Koreliacijos Skaičiavimas ---\n",
    "def calculate_correlation():\n",
    "    \"\"\"Apskaičiuoja koreliaciją tarp API ir kitų kintamųjų bei grąžina pilną lentelę.\"\"\"\n",
    "    # Koreliacijos matrica\n",
    "    correlation_matrix = weekly_data.corr()\n",
    "\n",
    "    # Gauta koreliacija su API (be pačio API stulpelio)\n",
    "    api_correlation = correlation_matrix['API'].drop('API')\n",
    "\n",
    "    # Lentelė su visais kintamaisiais, surūšiuotais pagal absoliutų dydį\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'Variable': api_correlation.index,\n",
    "        'Correlation': api_correlation.values\n",
    "    }).sort_values(by='Correlation', key=abs, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Grąžiname tvarkingą lentelę\n",
    "    return correlation_df\n",
    "\n",
    "def multivariate_forecast(horizon):\n",
    "    \"\"\"Atlieka multivariate forecasting naudodamas VAR su rankiniu būdu parinktais kintamaisiais.\"\"\"\n",
    "    # Rankiniu būdu parinkti kintamieji\n",
    "    manual_vars = ['API', 'no2', 'pm25', 'co', 'wind-gust']  # Čia įrašyk pasirinktus kintamuosius\n",
    "    \n",
    "    # Išspausdiname įtrauktus kintamuosius\n",
    "    print(f\"Selected Variables for Forecasting: {manual_vars}\")\n",
    "\n",
    "    # Paruošiame duomenis ir užtikriname, kad indeksas yra datų formatas\n",
    "    data = weekly_data[['Date'] + manual_vars].dropna()  # Include 'Date' in the filtered data\n",
    "    data['Date'] = pd.to_datetime(data['Date'])          # Ensure 'Date' is datetime\n",
    "    data = data.set_index('Date') \n",
    "    \n",
    "    # Mokymo duomenys\n",
    "    train = data.copy()\n",
    "\n",
    "    # Mokome VAR modelį\n",
    "    model_var = VAR(train)\n",
    "    results = model_var.fit(maxlags=5)\n",
    "\n",
    "    # Generuojame ateities datas\n",
    "    last_date = train.index[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(weeks=1), periods=horizon, freq='W')\n",
    "\n",
    "    # Prognozuojame ateities reikšmes\n",
    "    forecast = results.forecast(train.values[-5:], steps=horizon)\n",
    "    forecast_df = pd.DataFrame(forecast, index=future_dates, columns=manual_vars)\n",
    "\n",
    "    # Grafikas\n",
    "    fig = go.Figure()\n",
    "    # Original API\n",
    "    fig.add_trace(go.Scatter(x=train.index, y=train['API'], mode='lines', name=\"Training Data\"))\n",
    "    fig.add_trace(go.Scatter(x=future_dates, y=forecast_df['API'], mode='lines', name=\"Forecasted API\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Multivariate Forecasting into the Future\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"API Value\",\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    return fig, forecast_df"
   ],
   "id": "b47142278ba7c2cd",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T20:53:41.349036Z",
     "start_time": "2024-12-17T20:52:48.251655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Gradio Dashboard ---\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Dashboard\"):\n",
    "        # Filters\n",
    "        yearSelect = gr.CheckboxGroup(choices=dataTableAPI['Year'].unique().tolist(), label=\"Select Years:\")\n",
    "        \n",
    "        # Descriptive Statistics Table\n",
    "        statsTable = gr.Dataframe(label=\"Descriptive Statistics for All Cities\", interactive=True)\n",
    "        \n",
    "        # Monthly API Trend Line Chart\n",
    "        monthlyTrend = gr.Plot(label=\"Monthly API Trend for All Cities\", show_label=True)\n",
    "        \n",
    "        # Event Handlers\n",
    "        yearSelect.change(descriptive_stats_all, inputs=[yearSelect], outputs=[statsTable])\n",
    "        yearSelect.change(monthly_api_trend_all, inputs=[yearSelect], outputs=[monthlyTrend])\n",
    "        \n",
    "        # Load initial values\n",
    "        demo.load(descriptive_stats_all, inputs=[yearSelect], outputs=[statsTable])\n",
    "        demo.load(monthly_api_trend_all, inputs=[yearSelect], outputs=[monthlyTrend])\n",
    "\n",
    "    with gr.Tab(\"Variable Trends\"):\n",
    "        # Variable Selection and Line Chart\n",
    "        varSelect = gr.CheckboxGroup(choices=selectedVars, label=\"Select Variables:\")\n",
    "        yearSelectVars = gr.CheckboxGroup(choices=dataTableAPI['Year'].unique().tolist(), label=\"Select Years:\")\n",
    "        aggregationSelect = gr.Radio(choices=[\"Weekly\", \"Monthly\"], label=\"Select Aggregation Level:\", value=\"Monthly\")\n",
    "        varChart = gr.Plot(label=\"Variable Line Chart\")\n",
    "\n",
    "        # Event Handlers for Line Chart\n",
    "        varSelect.change(line_chart_selected_vars, inputs=[varSelect, yearSelectVars, aggregationSelect], outputs=[varChart])\n",
    "        yearSelectVars.change(line_chart_selected_vars, inputs=[varSelect, yearSelectVars, aggregationSelect], outputs=[varChart])\n",
    "        aggregationSelect.change(line_chart_selected_vars, inputs=[varSelect, yearSelectVars, aggregationSelect], outputs=[varChart])\n",
    "        \n",
    "    with gr.Tab(\"Weekly API Analysis\"):\n",
    "        # Filtrai\n",
    "        citySelectW = gr.CheckboxGroup(choices=dataTableAPI['City'].unique().tolist(), \n",
    "                                       label=\"Select Cities:\", \n",
    "                                       value=[\"Amsterdam\"])\n",
    "        yearSelectW = gr.CheckboxGroup(choices=dataTableAPI['Year'].unique().tolist(), \n",
    "                                       label=\"Select Years:\")\n",
    "        plotSelectW = gr.Radio(label=\"Select Plot Type:\", \n",
    "                               choices=['calplot', 'barchart'], \n",
    "                               value=\"calplot\")\n",
    "    \n",
    "        # Grafiko išvestis\n",
    "        weeklyPlot = gr.Plot(label=\"Weekly API Plot\", show_label=True)\n",
    "    \n",
    "        # Įvykiai filtrams\n",
    "        citySelectW.change(weekly_api_plot, inputs=[citySelectW, yearSelectW, plotSelectW], outputs=[weeklyPlot])\n",
    "        yearSelectW.change(weekly_api_plot, inputs=[citySelectW, yearSelectW, plotSelectW], outputs=[weeklyPlot])\n",
    "        plotSelectW.change(weekly_api_plot, inputs=[citySelectW, yearSelectW, plotSelectW], outputs=[weeklyPlot])\n",
    "    \n",
    "        # Pradinė būsena\n",
    "        demo.load(weekly_api_plot, inputs=[citySelectW, yearSelectW, plotSelectW], outputs=[weeklyPlot])\n",
    "    \n",
    "    with gr.Tab(\"WHO Threshold Analysis\"):\n",
    "        # Year Selection Checkbox Group\n",
    "        yearSelectWHO = gr.CheckboxGroup(choices=dataTableAPI['Year'].unique().tolist(), \n",
    "                                         label=\"Select Years:\", value=[dataTableAPI['Year'].min()])\n",
    "        # WHO Threshold Map Plot\n",
    "        mapPlotWHO = gr.Plot(label=\"WHO Threshold Exceedance Map\", show_label=True)\n",
    "        # Event Handler\n",
    "        yearSelectWHO.change(who_threshold_map, inputs=[yearSelectWHO], outputs=[mapPlotWHO])\n",
    "        # Pradinė būsena\n",
    "        demo.load(who_threshold_map, inputs=[yearSelectWHO], outputs=[mapPlotWHO])\n",
    "\n",
    "    with gr.Tab(\"Dominant Pollutant Analysis\"):\n",
    "        # Filtrai\n",
    "        yearSelectDom = gr.CheckboxGroup(choices=dataTableAPI['Year'].unique().tolist(), label=\"Select Years:\")\n",
    "        pieChartDom = gr.Plot(show_label=False, container=False)\n",
    "        barChartDom = gr.Plot(show_label=False, container=False)\n",
    "    \n",
    "        # Event Handlers\n",
    "        yearSelectDom.change(dominant_pollutant_summary, inputs=[yearSelectDom], outputs=[pieChartDom])\n",
    "        yearSelectDom.change(dominant_pollutant_trend, inputs=[yearSelectDom], outputs=[barChartDom])\n",
    "        \n",
    "        # Pradinė būsena\n",
    "        demo.load(dominant_pollutant_summary, inputs=[yearSelectDom], outputs=[pieChartDom])\n",
    "        demo.load(dominant_pollutant_trend, inputs=[yearSelectDom], outputs=[barChartDom])\n",
    "        \n",
    "    with gr.Tab(\"Correlation Analysis\"):\n",
    "        # Filtras pasirenkant kintamuosius\n",
    "        varSelectCorr = gr.CheckboxGroup(choices=selectedVars, label=\"Select Meteorological Variables:\")\n",
    "        corrHeatmap = gr.Plot(show_label=True, label=\"Pearson Correlation Heatmap\")\n",
    "        corrTable = gr.Dataframe(label=\"Correlation Matrix\", interactive=False)\n",
    "        pValueTable = gr.Dataframe(label=\"P-Values for Significance Test\", interactive=False)\n",
    "    \n",
    "        # Event Handler\n",
    "        varSelectCorr.change(pearson_correlation_analysis, inputs=[varSelectCorr], outputs=[corrHeatmap, corrTable, pValueTable])\n",
    "        \n",
    "    with gr.Tab(\"Forecast Preparation\"):\n",
    "            # Stationarity Check\n",
    "            adf_check = gr.Button(\"Check Stationarity\")\n",
    "            stationarity_output = gr.Dataframe(label=\"Stationarity Test Results\")\n",
    "            \n",
    "            # Distribution Check\n",
    "            dist_check = gr.Button(\"Check Distribution\")\n",
    "            dist_plot = gr.Plot(label=\"Distribution Check\")\n",
    "    \n",
    "            # Decomposition\n",
    "            decompose_btn = gr.Button(\"Decompose Time Series\")\n",
    "            decomposition_plot = gr.Plot(label=\"Time Series Decomposition\")\n",
    "    \n",
    "            # ACF/PACF\n",
    "            acf_pacf_btn = gr.Button(\"Plot ACF & PACF\")\n",
    "            acf_pacf_plot = gr.Plot(label=\"ACF and PACF\")\n",
    "    \n",
    "            # Event Handlers\n",
    "            adf_check.click(check_stationarity, outputs=stationarity_output)\n",
    "            dist_check.click(check_distribution, outputs=dist_plot)\n",
    "            decompose_btn.click(decompose_time_series, outputs=decomposition_plot)\n",
    "            acf_pacf_btn.click(plot_acf_pacf, outputs=acf_pacf_plot)\n",
    "\n",
    "    with gr.Tab(\"Forecasting & Backtesting\"):\n",
    "        forecast_horizon = gr.Slider(label=\"Forecast Horizon\", minimum=4, maximum=24, step=4, value=12)\n",
    "        forecast_table = gr.Dataframe(label=\"Forecast Results\")\n",
    "        backtesting_button = gr.Button(\"Run Backtesting\")\n",
    "        backtesting_table = gr.Dataframe(label=\"Backtesting Results\")\n",
    "        \n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=['Naive', 'Holt-Winter', 'ARIMA', 'Prophet'], \n",
    "            label=\"Select Forecast Model\", \n",
    "            value='Naive'\n",
    "        )\n",
    "        forecast_plot = gr.Plot(label=\"Forecast Visualization\")\n",
    "    \n",
    "        # Event Handlers\n",
    "        forecast_horizon.change(forecast_models, inputs=[forecast_horizon], outputs=forecast_table)\n",
    "        model_dropdown.change(plot_forecast, inputs=[model_dropdown, forecast_horizon], outputs=forecast_plot)\n",
    "        backtesting_button.click(backtesting_model, outputs=backtesting_table)\n",
    "        \n",
    "        # Gradio Tab for Multivariate Forecasting\n",
    "    with gr.Tab(\"Multivariate Forecasting\"):\n",
    "        # Koreliacijos Skaičiavimas\n",
    "        correlation_button = gr.Button(\"Calculate Correlation\")\n",
    "        correlation_table = gr.Dataframe(label=\"Top Variables by Correlation\")\n",
    "        \n",
    "        # Multivariate Forecasting\n",
    "        forecast_horizon_mv = gr.Slider(label=\"Forecast Horizon\", minimum=4, maximum=24, step=4, value=12)\n",
    "        mv_forecast_plot = gr.Plot(label=\"Multivariate Forecast Visualization\")\n",
    "        mv_forecast_table = gr.Dataframe(label=\"Forecast Results\")\n",
    "\n",
    "        # Event Handlers\n",
    "        correlation_button.click(calculate_correlation, outputs=correlation_table)\n",
    "        forecast_horizon_mv.change(multivariate_forecast, \n",
    "                                   inputs=[forecast_horizon_mv], \n",
    "                                   outputs=[mv_forecast_plot, mv_forecast_table])\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True, share=True)"
   ],
   "id": "bc39b931b0abe3a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7406e05a648d60bcb9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://7406e05a648d60bcb9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Filtering API Mean: 36.81244389027431\n",
      "After Filtering by Cities ['Amsterdam'] API Mean: 36.7043391521197\n",
      "After Dropping NaNs API Mean: 36.7043391521197\n",
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n",
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Variables for Forecasting: ['API', 'no2', 'pm25', 'co', 'wind-gust']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency W-SUN will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://7406e05a648d60bcb9.gradio.live\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "id": "d6eadeb3-4d88-48d7-afae-523f111b809a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T18:42:10.249144Z",
     "start_time": "2024-12-16T18:42:10.247536Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb06e183d56dc2e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
